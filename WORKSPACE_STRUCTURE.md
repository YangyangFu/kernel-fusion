# ğŸ“ Workspace Structure & CUDA Algorithm Development

## ğŸ— **Complete Workspace Structure**

```
kernel-fusion/
â”œâ”€â”€ ğŸ“¦ kernel_fusion/                    # Main Python package
â”‚   â”œâ”€â”€ __init__.py                     # Package initialization
â”‚   â””â”€â”€ ğŸ“‚ kernels/                     # Kernel implementations
â”‚       â”œâ”€â”€ __init__.py                 # Exports all kernels
â”‚       â”œâ”€â”€ cpu_kernels.py              # CPU reference implementations
â”‚       â”œâ”€â”€ cpu_layernorm.py            # CPU LayerNorm reference  
â”‚       â”œâ”€â”€ attention.py                # Legacy Triton attention
â”‚       â”œâ”€â”€ cuda_attention.py           # CUDA attention kernel
â”‚       â”œâ”€â”€ cuda_layernorm.py           # CUDA LayerNorm kernel
â”‚       â””â”€â”€ simple_cuda.py              # Basic CUDA learning examples
â”‚
â”œâ”€â”€ ğŸ““ examples/                        # Jupyter notebooks & examples
â”‚   â”œâ”€â”€ cuda_development_guide.ipynb   # Complete CUDA tutorial
â”‚   â”œâ”€â”€ layernorm_example.ipynb        # LayerNorm implementation example
â”‚   â”œâ”€â”€ macbook_development.ipynb      # MacBook Pro development
â”‚   â””â”€â”€ development_example.ipynb      # General development examples
â”‚
â”œâ”€â”€ ğŸ§ª tests/                          # Test suite
â”‚   â”œâ”€â”€ test_environment.py            # Environment validation
â”‚   â””â”€â”€ test_layernorm.py              # LayerNorm correctness & performance
â”‚
â”œâ”€â”€ ğŸ³ Docker Configuration             # Development environments
â”‚   â”œâ”€â”€ Dockerfile                     # GPU development (CUDA 12.1)
â”‚   â”œâ”€â”€ Dockerfile.cpu                 # CPU development (MacBook Pro)
â”‚   â”œâ”€â”€ docker-compose.yml             # GPU services
â”‚   â””â”€â”€ docker-compose.cpu.yml         # CPU services
â”‚
â”œâ”€â”€ ğŸ“‹ Requirements                     # Dependencies
â”‚   â”œâ”€â”€ requirements.txt               # General requirements
â”‚   â”œâ”€â”€ requirements-cuda.txt          # CUDA-specific packages
â”‚   â””â”€â”€ requirements-cpu.txt           # CPU-only packages
â”‚
â”œâ”€â”€ ğŸ›  Development Tools               # Setup & automation
â”‚   â”œâ”€â”€ Makefile                       # Common development tasks
â”‚   â”œâ”€â”€ setup-dev.sh                   # GPU environment setup
â”‚   â”œâ”€â”€ setup-mac.sh                   # MacBook Pro setup
â”‚   â””â”€â”€ setup.py                       # Package installation
â”‚
â””â”€â”€ ğŸ“š Documentation
    â”œâ”€â”€ README.md                       # Project overview
    â”œâ”€â”€ CUDA_DEVELOPMENT_GUIDE.md       # This comprehensive guide
    â””â”€â”€ .gitignore                      # Git configuration
```

## ğŸš€ **Step-by-Step: Adding a New CUDA Algorithm**

### 1ï¸âƒ£ **Plan Your Algorithm**
```bash
# Understand the math and parallelization strategy
# Example: For LayerNorm, we parallelize across sequence positions
```

### 2ï¸âƒ£ **Create CPU Reference**
```python
# File: kernel_fusion/kernels/cpu_my_algorithm.py
def cpu_my_algorithm(input_tensor):
    """CPU reference implementation for correctness testing"""
    # Implement your algorithm here
    return output_tensor
```

### 3ï¸âƒ£ **Implement CUDA Kernel**
```python
# File: kernel_fusion/kernels/cuda_my_algorithm.py
cuda_source = """
__global__ void my_algorithm_kernel(
    const float* input,
    float* output,
    const int n
) {
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    if (idx < n) {
        // Your CUDA implementation
        output[idx] = process(input[idx]);
    }
}
"""

class CUDAMyAlgorithm:
    def __init__(self):
        self.module = self._compile_cuda_kernel()
    
    def forward(self, input_tensor):
        if self.module and input_tensor.is_cuda:
            return self.module.my_algorithm(input_tensor)
        else:
            return cpu_fallback(input_tensor)
```

### 4ï¸âƒ£ **Add to Package**
```python
# File: kernel_fusion/kernels/__init__.py
try:
    from .cuda_my_algorithm import *
    print("My algorithm CUDA kernels available")
except ImportError:
    print("My algorithm CUDA kernels not available")
```

### 5ï¸âƒ£ **Create Tests**
```python
# File: tests/test_my_algorithm.py
def test_my_algorithm_correctness():
    # Test against CPU reference implementation
    
def test_my_algorithm_performance():
    # Benchmark against PyTorch/other implementations
```

### 6ï¸âƒ£ **Create Example Notebook**
```python
# File: examples/my_algorithm_example.ipynb
# - Algorithm explanation
# - Usage examples
# - Performance analysis
# - Visualization
```

## ğŸ”„ **Development Workflow**

### **Environment Setup**
```bash
# MacBook Pro (CPU development)
make setup          # Auto-detects macOS
make dev-mac         # Start CPU container
make jupyter-mac     # Launch Jupyter

# Linux/Cloud (GPU development)  
make setup           # Build GPU environment
make dev             # Start GPU container
make jupyter         # Launch Jupyter with CUDA
```

### **Development Loop**
```bash
# Inside container
cd /workspace

# 1. Edit kernel
vim kernel_fusion/kernels/cuda_my_algorithm.py

# 2. Test correctness
python -m pytest tests/test_my_algorithm.py -v

# 3. Benchmark performance
python -c "from tests.test_my_algorithm import benchmark_my_algorithm; benchmark_my_algorithm()"

# 4. Interactive development
jupyter lab --ip=0.0.0.0 --port=8888 --no-browser --allow-root
```

### **Code Quality**
```bash
make format          # Black code formatting
make lint            # Flake8 + mypy linting  
make test            # Run all tests
```

## ğŸ¯ **CUDA Kernel Design Patterns**

### **1. Element-wise Operations**
```cuda
__global__ void elementwise_kernel(const float* input, float* output, int n) {
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    if (idx < n) {
        output[idx] = process(input[idx]);
    }
}
// Grid: (n/block_size,), Block: (block_size,)
```

### **2. Reduction Operations (LayerNorm, Attention)**
```cuda
__global__ void reduction_kernel(const float* input, float* output, int n) {
    __shared__ float shared_data[BLOCK_SIZE];
    
    // Load data
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    shared_data[threadIdx.x] = (idx < n) ? input[idx] : 0.0f;
    __syncthreads();
    
    // Reduce within block
    for (int stride = blockDim.x / 2; stride > 0; stride >>= 1) {
        if (threadIdx.x < stride) {
            shared_data[threadIdx.x] += shared_data[threadIdx.x + stride];
        }
        __syncthreads();
    }
    
    // Store result
    if (threadIdx.x == 0) {
        output[blockIdx.x] = shared_data[0];
    }
}
```

### **3. Matrix Operations (Attention)**
```cuda
__global__ void matmul_kernel(
    const float* A, const float* B, float* C,
    int M, int N, int K
) {
    int row = blockIdx.y * blockDim.y + threadIdx.y;
    int col = blockIdx.x * blockDim.x + threadIdx.x;
    
    if (row < M && col < N) {
        float sum = 0.0f;
        for (int k = 0; k < K; k++) {
            sum += A[row * K + k] * B[k * N + col];
        }
        C[row * N + col] = sum;
    }
}
// Grid: (N/16, M/16), Block: (16, 16)
```

## ğŸ† **Best Practices**

### **Memory Optimization**
- âœ… **Coalescing**: Access consecutive memory locations
- âœ… **Shared Memory**: Cache frequently accessed data
- âœ… **Bank Conflicts**: Avoid shared memory bank conflicts
- âœ… **Occupancy**: Use 256+ threads per block

### **Code Organization**
- âœ… **Separation**: CPU fallback + CUDA implementation + high-level interface
- âœ… **Error Handling**: Graceful fallback when compilation fails
- âœ… **Documentation**: Clear docstrings and examples
- âœ… **Testing**: Both correctness and performance validation

### **Performance Tuning**
- âœ… **Profile First**: Use nsys/ncu to identify bottlenecks
- âœ… **Kernel Fusion**: Combine multiple operations
- âœ… **Data Types**: Consider fp16 for better throughput
- âœ… **Memory vs Compute**: Balance bandwidth vs compute intensity

## ğŸ›  **Available Tools**

### **Development Environment**
- **PyTorch C++ Extensions**: JIT compilation of CUDA kernels
- **Docker**: Consistent development environment
- **Jupyter**: Interactive development and experimentation

### **Profiling & Debugging**
- **nsys**: System-wide profiling (`nsys profile python script.py`)
- **ncu**: Kernel-level profiling (`ncu python script.py`) 
- **PyTorch Profiler**: Python-level profiling
- **cuda-gdb**: CUDA debugging

### **Testing & Validation**
- **pytest**: Automated testing framework
- **torch.allclose()**: Numerical correctness validation
- **Benchmarking**: Performance comparison utilities

## ğŸ“Š **Example: LayerNorm Implementation**

We've created a complete LayerNorm example that demonstrates:

- âœ… **CPU Reference**: `cpu_layernorm.py`
- âœ… **CUDA Implementation**: `cuda_layernorm.py` 
- âœ… **Comprehensive Tests**: `test_layernorm.py`
- âœ… **Interactive Example**: `layernorm_example.ipynb`
- âœ… **Performance Benchmarks**: Included in tests

This serves as a template for adding any new CUDA algorithm!

## ğŸ‰ **Ready to Develop!**

The workspace provides everything needed for CUDA kernel development:

1. **ğŸ MacBook Pro**: Start with CPU prototypes and learning
2. **â˜ï¸ Cloud GPU**: Move to actual CUDA development when ready  
3. **ğŸ”„ Automated Workflow**: Build, test, benchmark, and deploy
4. **ğŸ“š Comprehensive Examples**: Learn from working implementations

Follow the LayerNorm example to add your own CUDA algorithms!
