# Kernel Fusion

A collection of optimized fusion kernels for deep learning operations using **direct CUDA programming**. 
These kernels can be used with PyTorch through C++ extensions, serving as a standalone library for high-performance computing.

## ğŸš€ Features

- **High Performance**: Custom CUDA kernels optimized for fusion operations
- **PyTorch Integration**: Seamless integration with PyTorch tensors and autograd
- **Automatic Fallback**: CPU implementations when CUDA is not available
- **Type Support**: Full support for float16, float32, and float64 data types
- **Memory Efficient**: Reduced memory footprint through kernel fusion
- **Easy to Use**: High-level Python API with automatic device detection

## ğŸ“¦ Installation

### Prerequisites

- Python 3.8+
- PyTorch 1.12.0+
- CUDA Toolkit 11.0+ (for GPU acceleration)
- C++17 compatible compiler

### From Source

```bash
# Clone the repository
git clone https://github.com/yourusername/kernel-fusion.git
cd kernel-fusion

# Install in development mode
pip install -e .

# Or install with development dependencies
pip install -e ".[dev]"
```

### Verify Installation

```python
import kernel_fusion as kf
print(f"CUDA available: {kf.CUDA_AVAILABLE}")
print(f"Extension loaded: {kf.EXTENSION_LOADED}")
```

## ğŸ”¥ Quick Start

### Basic Operations

```python
import torch
import kernel_fusion as kf

# Create tensors
a = torch.randn(1024, 512, device='cuda')
b = torch.randn(1024, 512, device='cuda')

# Fused elementwise operations
result = kf.ops.elementwise_add_relu(a, b)  # Equivalent to torch.relu(a + b)
result = kf.ops.elementwise_mul_tanh(a, b)  # Equivalent to torch.tanh(a * b)

# Fused bias + activation
input_tensor = torch.randn(64, 128, device='cuda')
bias = torch.randn(128, device='cuda')
result = kf.ops.fused_bias_gelu(input_tensor, bias)  # Equivalent to torch.gelu(input + bias)
```

### Advanced Fusion Operations

```python
# Fused layer normalization + ReLU
input_tensor = torch.randn(32, 512, 768, device='cuda')
weight = torch.randn(768, device='cuda')
bias = torch.randn(768, device='cuda')

result = kf.ops.fused_layer_norm_relu(
    input_tensor, 
    normalized_shape=(768,), 
    weight=weight, 
    bias=bias
)

# Fused attention scores
query = torch.randn(4, 32, 64, device='cuda')  # [batch, seq_len, dim]
key = torch.randn(4, 32, 64, device='cuda')
scale = 1.0 / (64 ** 0.5)

scores = kf.ops.fused_attention_score(query, key, scale)
```

### Direct Kernel Access

For advanced users who need fine-grained control:

```python
# Direct access to individual kernels
result = kf.kernels.elementwise.add_relu(a, b)
result = kf.kernels.fusion.layer_norm_relu(input_tensor, normalized_shape, weight, bias)
result = kf.kernels.reduction.sum_squared(input_tensor, dim=-1)
```

## ğŸ—ï¸ Library Structure

```
kernel-fusion/
â”œâ”€â”€ kernel_fusion/           # Python package
â”‚   â”œâ”€â”€ __init__.py         # Package initialization and CUDA detection
â”‚   â”œâ”€â”€ ops.py              # High-level operation APIs with fallbacks
â”‚   â””â”€â”€ kernels.py          # Direct kernel access interface
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ cpp/                # C++ binding layer
â”‚   â”‚   â”œâ”€â”€ bindings.cpp    # Python bindings using pybind11
â”‚   â”‚   â”œâ”€â”€ kernels/        # C++ kernel dispatchers
â”‚   â”‚   â””â”€â”€ utils/          # C++ utilities
â”‚   â””â”€â”€ cuda/               # CUDA implementation
â”‚       â”œâ”€â”€ kernels/        # CUDA kernel implementations
â”‚       â””â”€â”€ utils/          # CUDA utilities and helpers
â”œâ”€â”€ tests/                  # Comprehensive test suite
â”œâ”€â”€ examples/               # Usage examples and benchmarks
â””â”€â”€ setup.py               # Build configuration
```

## ğŸ§ª Available Operations

### Elementwise Operations
- `elementwise_add_relu(a, b)` - Fused addition + ReLU
- `elementwise_mul_tanh(a, b)` - Fused multiplication + Tanh  
- `fused_bias_gelu(input, bias)` - Fused bias addition + GELU

### Reduction Operations
- `reduce_sum_squared(input, dim)` - Fused square + sum reduction
- `reduce_mean_abs(input, dim)` - Fused absolute value + mean reduction

### Complex Fusion Operations
- `fused_layer_norm_relu(input, ...)` - Fused layer normalization + ReLU
- `fused_gelu_dropout(input, p, training)` - Fused GELU + dropout
- `fused_attention_score(query, key, scale)` - Fused attention score computation

## ğŸ“Š Performance

Example performance improvements on NVIDIA A100:

| Operation | Standard PyTorch | Kernel Fusion | Speedup |
|-----------|------------------|---------------|---------|
| Add + ReLU | 1.23ms | 0.84ms | 1.46x |
| Layer Norm + ReLU | 2.45ms | 1.67ms | 1.47x |
| Bias + GELU | 0.98ms | 0.71ms | 1.38x |
| Attention Scores | 3.21ms | 2.18ms | 1.47x |

*Benchmarks on tensors of size (2048, 2048) with float32 precision.*

## ğŸ”§ Development

### Building from Source

```bash
# Install development dependencies
pip install -e ".[dev]"

# Run tests
pytest tests/

# Run specific test categories
pytest tests/ -m "not slow"  # Skip slow tests
pytest tests/ -m "cuda"      # Only CUDA tests

# Format code
black kernel_fusion/ tests/ examples/

# Type checking
mypy kernel_fusion/
```

### Adding New Kernels

1. **CUDA Implementation**: Add kernel in `src/cuda/kernels/`
2. **C++ Binding**: Add dispatcher in `src/cpp/kernels/`
3. **Python Interface**: Add high-level API in `kernel_fusion/ops.py`
4. **Tests**: Add comprehensive tests in `tests/`

### Testing

```bash
# Run all tests
pytest

# Run with coverage
pytest --cov=kernel_fusion

# Benchmark performance
pytest tests/test_operations.py::TestPerformance -v
```

## ğŸ“š Examples

- [`basic_usage.py`](examples/basic_usage.py) - Simple examples of all operations
- [`transformer_integration.py`](examples/transformer_integration.py) - Integration with transformer models
- [`benchmarks.py`](examples/benchmarks.py) - Performance benchmarking tools

## ğŸ¤ Contributing

Contributions are welcome! Please see our [Contributing Guidelines](CONTRIBUTING.md) for details.

1. Fork the repository
2. Create a feature branch
3. Add tests for new functionality
4. Ensure all tests pass
5. Submit a pull request

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## ğŸ™ Acknowledgments

- PyTorch team for the excellent C++ extension framework
- NVIDIA for CUDA and optimization resources
- Community contributors and testers

