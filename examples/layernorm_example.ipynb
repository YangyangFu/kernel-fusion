{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dfeb62ae",
   "metadata": {},
   "source": [
    "# LayerNorm CUDA Example\n",
    "\n",
    "This notebook demonstrates the complete process of adding a CUDA algorithm (LayerNorm) to the workspace.\n",
    "\n",
    "## What is LayerNorm?\n",
    "\n",
    "LayerNormalization normalizes inputs across the feature dimension:\n",
    "\n",
    "$$\\text{LayerNorm}(x) = \\gamma \\frac{x - \\mu}{\\sqrt{\\sigma^2 + \\epsilon}} + \\beta$$\n",
    "\n",
    "Where:\n",
    "- $\\mu$ is the mean across features\n",
    "- $\\sigma^2$ is the variance across features  \n",
    "- $\\gamma$ and $\\beta$ are learnable parameters\n",
    "- $\\epsilon$ is a small constant for numerical stability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80548e6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA device: {torch.cuda.get_device_name()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efa647e5",
   "metadata": {},
   "source": [
    "## Step 1: CPU Reference Implementation\n",
    "\n",
    "Always start with a CPU implementation to understand the algorithm:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26ee47cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def manual_layernorm(x, weight=None, bias=None, eps=1e-5):\n",
    "    \"\"\"\n",
    "    Manual implementation to understand the algorithm\n",
    "    \"\"\"\n",
    "    print(f\"Input shape: {x.shape}\")\n",
    "    \n",
    "    # Step 1: Calculate mean across the last dimension (features)\n",
    "    mean = x.mean(dim=-1, keepdim=True)\n",
    "    print(f\"Mean shape: {mean.shape}\")\n",
    "    \n",
    "    # Step 2: Calculate variance\n",
    "    variance = x.var(dim=-1, keepdim=True, unbiased=False)\n",
    "    print(f\"Variance shape: {variance.shape}\")\n",
    "    \n",
    "    # Step 3: Normalize\n",
    "    x_norm = (x - mean) / torch.sqrt(variance + eps)\n",
    "    print(f\"Normalized shape: {x_norm.shape}\")\n",
    "    \n",
    "    # Step 4: Scale and shift (if provided)\n",
    "    if weight is not None:\n",
    "        x_norm = x_norm * weight\n",
    "        print(f\"After scaling: {x_norm.shape}\")\n",
    "    \n",
    "    if bias is not None:\n",
    "        x_norm = x_norm + bias\n",
    "        print(f\"After bias: {x_norm.shape}\")\n",
    "    \n",
    "    return x_norm\n",
    "\n",
    "# Test the manual implementation\n",
    "batch_size, seq_len, hidden_size = 2, 4, 6\n",
    "x = torch.randn(batch_size, seq_len, hidden_size)\n",
    "weight = torch.randn(hidden_size)\n",
    "bias = torch.randn(hidden_size)\n",
    "\n",
    "print(\"=== Manual LayerNorm ===\")\n",
    "output = manual_layernorm(x, weight, bias)\n",
    "print(f\"\\nOutput shape: {output.shape}\")\n",
    "print(f\"Output mean per feature: {output.mean(dim=(0,1))}\")\n",
    "print(f\"Output std per feature: {output.std(dim=(0,1))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c38e377",
   "metadata": {},
   "source": [
    "## Step 2: Verify Against PyTorch Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec24c134",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare with PyTorch's LayerNorm\n",
    "layer_norm = torch.nn.LayerNorm(hidden_size)\n",
    "layer_norm.weight.data = weight.clone()\n",
    "layer_norm.bias.data = bias.clone()\n",
    "\n",
    "pytorch_output = layer_norm(x)\n",
    "manual_output = manual_layernorm(x, weight, bias)\n",
    "\n",
    "print(f\"Manual implementation matches PyTorch: {torch.allclose(manual_output, pytorch_output, atol=1e-6)}\")\n",
    "print(f\"Max difference: {torch.max(torch.abs(manual_output - pytorch_output))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6efeced",
   "metadata": {},
   "source": [
    "## Step 3: Understanding CUDA Parallelization Strategy\n",
    "\n",
    "For LayerNorm, we need to think about how to parallelize:\n",
    "\n",
    "1. **Each sequence position** can be processed independently\n",
    "2. **Within each position**, we need to:\n",
    "   - Calculate mean and variance (reduction across features)\n",
    "   - Apply normalization element-wise\n",
    "\n",
    "### CUDA Kernel Design:\n",
    "- **Grid**: `(batch_size, seq_len)` - one block per sequence position\n",
    "- **Block**: 256 threads to process features in parallel\n",
    "- **Shared Memory**: For efficient reductions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "489dea1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze the computation pattern\n",
    "def analyze_layernorm_computation(batch_size, seq_len, hidden_size):\n",
    "    print(f\"LayerNorm Analysis for shape [{batch_size}, {seq_len}, {hidden_size}]:\")\n",
    "    print(f\"  Total sequence positions: {batch_size * seq_len}\")\n",
    "    print(f\"  Features per position: {hidden_size}\")\n",
    "    print(f\"  Total elements: {batch_size * seq_len * hidden_size:,}\")\n",
    "    \n",
    "    # Memory analysis\n",
    "    input_memory = batch_size * seq_len * hidden_size * 4  # 4 bytes per float\n",
    "    stats_memory = batch_size * seq_len * 2 * 4  # mean and std\n",
    "    weight_bias_memory = hidden_size * 2 * 4\n",
    "    \n",
    "    print(f\"\\nMemory Usage:\")\n",
    "    print(f\"  Input tensor: {input_memory / 1024**2:.2f} MB\")\n",
    "    print(f\"  Statistics (mean/std): {stats_memory / 1024**2:.2f} MB\")\n",
    "    print(f\"  Weight/Bias: {weight_bias_memory / 1024:.2f} KB\")\n",
    "    \n",
    "    # Parallelization strategy\n",
    "    print(f\"\\nCUDA Parallelization:\")\n",
    "    print(f\"  Grid size: ({batch_size}, {seq_len}) = {batch_size * seq_len} blocks\")\n",
    "    print(f\"  Block size: 256 threads\")\n",
    "    print(f\"  Threads per feature: {256 / hidden_size:.2f}\" if hidden_size <= 256 else f\"  Features per thread: {hidden_size / 256:.2f}\")\n",
    "\n",
    "# Analyze different sizes\n",
    "sizes = [(2, 128, 768), (32, 512, 1024), (64, 1024, 2048)]\n",
    "for size in sizes:\n",
    "    analyze_layernorm_computation(*size)\n",
    "    print(\"\\n\" + \"=\"*50 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70573e07",
   "metadata": {},
   "source": [
    "## Step 4: Test Our CUDA Implementation (if available)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a40543e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try to import our CUDA implementation\n",
    "try:\n",
    "    from kernel_fusion.kernels.cuda_layernorm import layernorm, CUDALayerNorm\n",
    "    from kernel_fusion.kernels.cpu_layernorm import cpu_layernorm\n",
    "    CUDA_IMPL_AVAILABLE = True\n",
    "    print(\"✅ CUDA LayerNorm implementation available!\")\n",
    "except ImportError as e:\n",
    "    CUDA_IMPL_AVAILABLE = False\n",
    "    print(f\"❌ CUDA LayerNorm not available: {e}\")\n",
    "    print(\"   This is expected on CPU-only systems or before compilation\")\n",
    "\n",
    "if CUDA_IMPL_AVAILABLE:\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"Testing on device: {device}\")\n",
    "    \n",
    "    # Test our implementation\n",
    "    x_test = torch.randn(4, 8, 128, device=device)\n",
    "    weight_test = torch.randn(128, device=device)\n",
    "    bias_test = torch.randn(128, device=device)\n",
    "    \n",
    "    # Our implementation\n",
    "    our_output = layernorm(x_test, weight_test, bias_test)\n",
    "    \n",
    "    # PyTorch reference\n",
    "    layer_norm_ref = torch.nn.LayerNorm(128, device=device)\n",
    "    layer_norm_ref.weight.data = weight_test.clone()\n",
    "    layer_norm_ref.bias.data = bias_test.clone()\n",
    "    pytorch_output = layer_norm_ref(x_test)\n",
    "    \n",
    "    # Compare\n",
    "    print(f\"\\nCorrectness check:\")\n",
    "    print(f\"  Our output shape: {our_output.shape}\")\n",
    "    print(f\"  PyTorch output shape: {pytorch_output.shape}\")\n",
    "    print(f\"  Max difference: {torch.max(torch.abs(our_output - pytorch_output))}\")\n",
    "    print(f\"  Outputs match: {torch.allclose(our_output, pytorch_output, atol=1e-4)}\")\n",
    "else:\n",
    "    print(\"\\n💡 To test the CUDA implementation:\")\n",
    "    print(\"   1. Build the Docker container with GPU support\")\n",
    "    print(\"   2. Run this notebook in the GPU environment\")\n",
    "    print(\"   3. The CUDA kernels will compile on first use\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1507c08d",
   "metadata": {},
   "source": [
    "## Step 5: Performance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a7b64d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_layernorm_implementations():\n",
    "    \"\"\"Benchmark different LayerNorm implementations\"\"\"\n",
    "    \n",
    "    if not torch.cuda.is_available():\n",
    "        print(\"CUDA not available - running CPU benchmark only\")\n",
    "        device = torch.device('cpu')\n",
    "    else:\n",
    "        device = torch.device('cuda')\n",
    "    \n",
    "    # Test configuration\n",
    "    batch_size, seq_len, hidden_size = 32, 512, 768\n",
    "    x = torch.randn(batch_size, seq_len, hidden_size, device=device)\n",
    "    weight = torch.randn(hidden_size, device=device)\n",
    "    bias = torch.randn(hidden_size, device=device)\n",
    "    \n",
    "    print(f\"Benchmarking LayerNorm on {device}\")\n",
    "    print(f\"Input shape: [{batch_size}, {seq_len}, {hidden_size}]\")\n",
    "    print(\"\\nImplementations to test:\")\n",
    "    \n",
    "    implementations = {}\n",
    "    \n",
    "    # PyTorch implementation\n",
    "    layer_norm_pytorch = torch.nn.LayerNorm(hidden_size, device=device)\n",
    "    layer_norm_pytorch.weight.data = weight.clone()\n",
    "    layer_norm_pytorch.bias.data = bias.clone()\n",
    "    implementations['PyTorch'] = lambda: layer_norm_pytorch(x)\n",
    "    \n",
    "    # Manual implementation  \n",
    "    implementations['Manual'] = lambda: manual_layernorm(x, weight, bias)\n",
    "    \n",
    "    # Our CUDA implementation (if available)\n",
    "    if CUDA_IMPL_AVAILABLE:\n",
    "        implementations['Our CUDA'] = lambda: layernorm(x, weight, bias)\n",
    "    \n",
    "    # Benchmark each implementation\n",
    "    results = {}\n",
    "    for name, func in implementations.items():\n",
    "        print(f\"  - {name}\")\n",
    "        \n",
    "        # Warmup\n",
    "        for _ in range(5):\n",
    "            _ = func()\n",
    "        \n",
    "        if device.type == 'cuda':\n",
    "            torch.cuda.synchronize()\n",
    "        \n",
    "        # Benchmark\n",
    "        times = []\n",
    "        for _ in range(20):\n",
    "            start = time.time()\n",
    "            _ = func()\n",
    "            if device.type == 'cuda':\n",
    "                torch.cuda.synchronize()\n",
    "            times.append(time.time() - start)\n",
    "        \n",
    "        avg_time = np.mean(times[5:]) * 1000  # Skip first 5, convert to ms\n",
    "        results[name] = avg_time\n",
    "    \n",
    "    # Display results\n",
    "    print(\"\\n\" + \"=\"*40)\n",
    "    print(\"Benchmark Results:\")\n",
    "    print(\"=\"*40)\n",
    "    \n",
    "    baseline = results['PyTorch']\n",
    "    for name, time_ms in results.items():\n",
    "        speedup = baseline / time_ms\n",
    "        print(f\"{name:<15}: {time_ms:6.2f}ms  ({speedup:4.2f}x)\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Run benchmark\n",
    "benchmark_results = benchmark_layernorm_implementations()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97a8eff7",
   "metadata": {},
   "source": [
    "## Step 6: Visualize LayerNorm Effects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a9c31a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize what LayerNorm does to the data\n",
    "def visualize_layernorm_effects():\n",
    "    # Create sample data with different scales\n",
    "    torch.manual_seed(42)\n",
    "    batch_size, seq_len, hidden_size = 1, 1, 1000\n",
    "    \n",
    "    # Create data with different means and scales\n",
    "    x1 = torch.randn(batch_size, seq_len, hidden_size) * 0.1 + 2.0   # Small variance, positive mean\n",
    "    x2 = torch.randn(batch_size, seq_len, hidden_size) * 5.0 - 1.0   # Large variance, negative mean\n",
    "    x3 = torch.randn(batch_size, seq_len, hidden_size) * 1.0 + 0.0   # Normal distribution\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "    \n",
    "    # Plot original distributions\n",
    "    axes[0, 0].hist(x1.flatten().numpy(), bins=50, alpha=0.7, color='red')\n",
    "    axes[0, 0].set_title('Original: Small var, +mean')\n",
    "    axes[0, 0].set_ylabel('Frequency')\n",
    "    \n",
    "    axes[0, 1].hist(x2.flatten().numpy(), bins=50, alpha=0.7, color='blue')\n",
    "    axes[0, 1].set_title('Original: Large var, -mean')\n",
    "    \n",
    "    axes[0, 2].hist(x3.flatten().numpy(), bins=50, alpha=0.7, color='green')\n",
    "    axes[0, 2].set_title('Original: Normal distribution')\n",
    "    \n",
    "    # Apply LayerNorm\n",
    "    y1 = manual_layernorm(x1)\n",
    "    y2 = manual_layernorm(x2)\n",
    "    y3 = manual_layernorm(x3)\n",
    "    \n",
    "    # Plot normalized distributions\n",
    "    axes[1, 0].hist(y1.flatten().numpy(), bins=50, alpha=0.7, color='red')\n",
    "    axes[1, 0].set_title('After LayerNorm')\n",
    "    axes[1, 0].set_xlabel('Value')\n",
    "    axes[1, 0].set_ylabel('Frequency')\n",
    "    \n",
    "    axes[1, 1].hist(y2.flatten().numpy(), bins=50, alpha=0.7, color='blue')\n",
    "    axes[1, 1].set_title('After LayerNorm')\n",
    "    axes[1, 1].set_xlabel('Value')\n",
    "    \n",
    "    axes[1, 2].hist(y3.flatten().numpy(), bins=50, alpha=0.7, color='green')\n",
    "    axes[1, 2].set_title('After LayerNorm')\n",
    "    axes[1, 2].set_xlabel('Value')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print statistics\n",
    "    print(\"Statistics before LayerNorm:\")\n",
    "    print(f\"  Dataset 1: mean={x1.mean():.3f}, std={x1.std():.3f}\")\n",
    "    print(f\"  Dataset 2: mean={x2.mean():.3f}, std={x2.std():.3f}\")\n",
    "    print(f\"  Dataset 3: mean={x3.mean():.3f}, std={x3.std():.3f}\")\n",
    "    \n",
    "    print(\"\\nStatistics after LayerNorm:\")\n",
    "    print(f\"  Dataset 1: mean={y1.mean():.3f}, std={y1.std():.3f}\")\n",
    "    print(f\"  Dataset 2: mean={y2.mean():.3f}, std={y2.std():.3f}\")\n",
    "    print(f\"  Dataset 3: mean={y3.mean():.3f}, std={y3.std():.3f}\")\n",
    "\n",
    "visualize_layernorm_effects()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8d9a45d",
   "metadata": {},
   "source": [
    "## Summary: LayerNorm CUDA Development Process\n",
    "\n",
    "We've demonstrated the complete process of adding a CUDA algorithm:\n",
    "\n",
    "### ✅ **What we accomplished:**\n",
    "\n",
    "1. **📊 Algorithm Understanding**: Broke down LayerNorm mathematically\n",
    "2. **💻 CPU Prototype**: Implemented and verified correctness\n",
    "3. **🚀 CUDA Design**: Planned parallelization strategy\n",
    "4. **⚡ Performance Analysis**: Benchmarked implementations\n",
    "5. **🧪 Testing**: Created comprehensive test suite\n",
    "6. **📈 Visualization**: Understood the algorithm's effects\n",
    "\n",
    "### 🔧 **Files Created:**\n",
    "- `kernel_fusion/kernels/cpu_layernorm.py` - CPU reference\n",
    "- `kernel_fusion/kernels/cuda_layernorm.py` - CUDA implementation\n",
    "- `tests/test_layernorm.py` - Test suite\n",
    "- This notebook - Documentation and examples\n",
    "\n",
    "### 🎯 **Key CUDA Concepts Demonstrated:**\n",
    "- **Grid/Block Organization**: One block per sequence position\n",
    "- **Shared Memory**: For efficient reductions\n",
    "- **Memory Coalescing**: Accessing consecutive elements\n",
    "- **Fallback Mechanisms**: CPU implementation when CUDA fails\n",
    "\n",
    "### 🚀 **Next Steps:**\n",
    "1. **Compile and test** in GPU environment\n",
    "2. **Profile and optimize** using nsys/ncu\n",
    "3. **Add more kernels** following this pattern\n",
    "4. **Benchmark against** production libraries\n",
    "\n",
    "This process can be applied to any CUDA algorithm you want to add to the workspace!"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
