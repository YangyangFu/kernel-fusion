{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "06323496",
   "metadata": {},
   "source": [
    "# Kernel Fusion Development Example\n",
    "\n",
    "This notebook demonstrates the development environment setup for fusion kernels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9358ebaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import triton\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA device: {torch.cuda.get_device_name()}\")\n",
    "    print(f\"CUDA version: {torch.version.cuda}\")\n",
    "print(f\"Triton version: {triton.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10dfca56",
   "metadata": {},
   "source": [
    "## Test Basic GPU Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa38757a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test basic GPU operations\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Create tensors on GPU\n",
    "x = torch.randn(1000, 1000, device=device)\n",
    "y = torch.randn(1000, 1000, device=device)\n",
    "\n",
    "# Perform matrix multiplication\n",
    "z = torch.matmul(x, y)\n",
    "print(f\"Matrix multiplication result shape: {z.shape}\")\n",
    "print(f\"Result tensor device: {z.device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7438ce63",
   "metadata": {},
   "source": [
    "## Example: Simple Triton Kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f40c4ecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import triton.language as tl\n",
    "\n",
    "@triton.jit\n",
    "def add_kernel(x_ptr, y_ptr, output_ptr, n_elements, BLOCK_SIZE: tl.constexpr):\n",
    "    \"\"\"\n",
    "    Simple element-wise addition kernel\n",
    "    \"\"\"\n",
    "    # Get the program ID\n",
    "    pid = tl.program_id(axis=0)\n",
    "    \n",
    "    # Compute block start\n",
    "    block_start = pid * BLOCK_SIZE\n",
    "    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n",
    "    \n",
    "    # Create mask for bounds checking\n",
    "    mask = offsets < n_elements\n",
    "    \n",
    "    # Load data\n",
    "    x = tl.load(x_ptr + offsets, mask=mask)\n",
    "    y = tl.load(y_ptr + offsets, mask=mask)\n",
    "    \n",
    "    # Compute output\n",
    "    output = x + y\n",
    "    \n",
    "    # Store result\n",
    "    tl.store(output_ptr + offsets, output, mask=mask)\n",
    "\n",
    "\n",
    "def add_triton(x: torch.Tensor, y: torch.Tensor):\n",
    "    \"\"\"\n",
    "    Wrapper function for Triton addition kernel\n",
    "    \"\"\"\n",
    "    output = torch.empty_like(x)\n",
    "    assert x.is_cuda and y.is_cuda\n",
    "    n_elements = output.numel()\n",
    "    \n",
    "    # The BLOCK_SIZE must be a power of 2\n",
    "    BLOCK_SIZE = 1024\n",
    "    grid = (triton.cdiv(n_elements, BLOCK_SIZE),)\n",
    "    \n",
    "    add_kernel[grid](x, y, output, n_elements, BLOCK_SIZE=BLOCK_SIZE)\n",
    "    return output\n",
    "\n",
    "# Test the kernel\n",
    "if torch.cuda.is_available():\n",
    "    a = torch.randn(1000, device='cuda')\n",
    "    b = torch.randn(1000, device='cuda')\n",
    "    \n",
    "    # Compare Triton vs PyTorch\n",
    "    result_triton = add_triton(a, b)\n",
    "    result_torch = a + b\n",
    "    \n",
    "    print(f\"Max difference: {torch.max(torch.abs(result_triton - result_torch)).item()}\")\n",
    "    print(\"Triton kernel test passed!\" if torch.allclose(result_triton, result_torch) else \"Test failed!\")\n",
    "else:\n",
    "    print(\"CUDA not available, skipping Triton kernel test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6969fb73",
   "metadata": {},
   "source": [
    "## Benchmarking Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a027c58e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def benchmark_function(func, *args, warmup=10, repeat=100):\n",
    "    \"\"\"\n",
    "    Simple benchmarking utility\n",
    "    \"\"\"\n",
    "    # Warmup\n",
    "    for _ in range(warmup):\n",
    "        _ = func(*args)\n",
    "    \n",
    "    # Synchronize GPU\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.synchronize()\n",
    "    \n",
    "    # Benchmark\n",
    "    start_time = time.time()\n",
    "    for _ in range(repeat):\n",
    "        _ = func(*args)\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.synchronize()\n",
    "    \n",
    "    end_time = time.time()\n",
    "    avg_time = (end_time - start_time) / repeat\n",
    "    \n",
    "    return avg_time * 1000  # Return in milliseconds\n",
    "\n",
    "# Example benchmark\n",
    "if torch.cuda.is_available():\n",
    "    sizes = [1000, 2000, 4000, 8000]\n",
    "    torch_times = []\n",
    "    triton_times = []\n",
    "    \n",
    "    for size in sizes:\n",
    "        a = torch.randn(size, device='cuda')\n",
    "        b = torch.randn(size, device='cuda')\n",
    "        \n",
    "        torch_time = benchmark_function(torch.add, a, b)\n",
    "        triton_time = benchmark_function(add_triton, a, b)\n",
    "        \n",
    "        torch_times.append(torch_time)\n",
    "        triton_times.append(triton_time)\n",
    "        \n",
    "        print(f\"Size {size}: PyTorch {torch_time:.3f}ms, Triton {triton_time:.3f}ms\")\n",
    "    \n",
    "    # Plot results\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(sizes, torch_times, 'b-o', label='PyTorch')\n",
    "    plt.plot(sizes, triton_times, 'r-s', label='Triton')\n",
    "    plt.xlabel('Tensor Size')\n",
    "    plt.ylabel('Time (ms)')\n",
    "    plt.title('Performance Comparison: PyTorch vs Triton')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"CUDA not available, skipping benchmark\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
